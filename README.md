# LangChain + Gemini + MCP (Tool Server)

Este projeto demonstra como criar um chain com LangChain usando o modelo **gemini-2.0-flash** do Google e integr√°-lo com ferramentas customizadas usando o **MCP** (Multi-Chain Protocol), via conex√£o `stdio`.

## ‚ú® Funcionalidade

A chain √© capaz de:

- Interpretar linguagem natural com o Gemini.
- Usar ferramentas externas via MCP ‚Äî neste exemplo, uma calculadora de express√µes matem√°ticas.
- Executar localmente um servidor de ferramentas que se conecta √† chain automaticamente.

## üîß Requisitos

- Python 3.10+
- [uv](https://github.com/astral-sh/uv) (ou `pip` tradicional)

## üì¶ Instala√ß√£o

#### Com uv (recomendado)

```bash
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

#### Ou com pip tradicional

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

#### Crie um arquivo .env:

```bash
GOOGLE_API_KEY=sua_chave_google_api
```

## üöÄ Execu√ß√£o

### 1. Execute o servidor de ferramentas (MCP)

Primeiro, execute o servidor de ferramentas. Este servidor vai processar as express√µes matem√°ticas.

```bash
python server.py
```

### 2. Execute o cliente

Em seguida, execute o cliente, que se conecta ao servidor e faz as requisi√ß√µes, passando a pergunta para a chain e recebendo o resultado do c√°lculo.

```bash
python client.py
```

### Como Funciona?

- O chain usa o modelo Gemini-Pro do Google para interpretar a linguagem natural.
- Uma LLMChain √© criada usando o modelo e um PromptTemplate para extrair a express√£o matem√°tica de uma pergunta.
- A express√£o extra√≠da √© ent√£o passada para uma ferramenta de c√°lculo via o protocolo MCP.
- O servidor MCP calcula a express√£o matem√°tica e retorna o resultado.
